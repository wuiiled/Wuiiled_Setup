#!/bin/bash
# å‡½æ•°ï¼šç”Ÿæˆ ADs_merged.txt
generate_ads_merged() {
      # æœ€ç»ˆè¾“å‡ºæ–‡ä»¶
    OUTPUT_FILE="ADs_merged.txt"

    # ä¸´æ—¶å·¥ä½œç›®å½•
    WORK_DIR=$(mktemp -d)
    trap "rm -rf ${WORK_DIR}" EXIT

    # æ‹¦æˆªè§„åˆ™æº (Blocklist URLs)
    BLOCK_URLS=(
        "https://raw.githubusercontent.com/pmkol/easymosdns/rules/ad_domain_list.txt"
        "https://raw.githubusercontent.com/wuiiled/Wuiiled_Setup/refs/heads/master/rules/Custom_Reject.txt"
        "https://adrules.top/adrules_domainset.txt"
        "https://adguardteam.github.io/HostlistsRegistry/assets/filter_1.txt"
        "https://adguardteam.github.io/HostlistsRegistry/assets/filter_3.txt"
        "https://adguardteam.github.io/HostlistsRegistry/assets/filter_4.txt"
        "https://raw.githubusercontent.com/TG-Twilight/AWAvenue-Ads-Rule/main/Filters/AWAvenue-Ads-Rule-Surge-RULE-SET.list"
        "https://raw.githubusercontent.com/ForestL18/rules-dat/mihomo/geo/classical/pcdn.list"
        "https://raw.githubusercontent.com/ForestL18/rules-dat/refs/heads/mihomo/geo/classical/reject.list"
        "https://a.dove.isdumb.one/pihole.txt"
    )

    # ç™½åå•æº (Allowlist URLs)
    ALLOW_URLS=(
        "https://raw.githubusercontent.com/Cats-Team/AdRules/refs/heads/script/script/allowlist.txt"
        "https://raw.githubusercontent.com/mawenjian/china-cdn-domain-whitelist/refs/heads/master/china-cdn-domain-whitelist.txt"
        "https://raw.githubusercontent.com/zoonderkins/blahdns/refs/heads/master/hosts/whitelist.txt"
    )

    # ================= åŠŸèƒ½å‡½æ•° =================

    download_files() {
        local urls=("$@")
        local output_file=$1
        # ç§»é™¤ç¬¬ä¸€ä¸ªå‚æ•°(è¾“å‡ºæ–‡ä»¶å)ï¼Œä¿ç•™å‰©ä¸‹çš„ä½œä¸ºURLæ•°ç»„
        shift
        local url_list=("$@")
        
        for url in "${url_list[@]}"; do
            echo "â¬‡ï¸  æ­£åœ¨ä¸‹è½½: $url"
            curl -sL --connect-timeout 10 --retry 3 "$url" >> "$output_file"
            echo "" >> "$output_file" # ç¡®ä¿æ–‡ä»¶æœ«å°¾æœ‰æ¢è¡Œï¼Œé˜²æ­¢æ‹¼æ¥é”™è¯¯
        done
    }

    clean_domains() {
        local input_file=$1
        local output_file=$2

        echo "ğŸ§¹ æ­£åœ¨æ¸…æ´—è§„åˆ™..."
        
        # è§£é‡Š sed/grep ç®¡é“æ“ä½œï¼š
        # 1. dos2unix: ç§»é™¤ Windows æ¢è¡Œç¬¦ \r
        # 2. grep -v: ç§»é™¤åŒ…å« DOMAIN-KEYWORD çš„è¡Œ
        # 3. sed ç§»é™¤æ³¨é‡Š: ç§»é™¤è¡Œé¦–çš„ ! å’Œ #
        # 4. sed ç§»é™¤ä¿®é¥°ç¬¦: ç§»é™¤ || å’Œ ^
        # 5. sed ç§»é™¤å‰ç¼€: ç§»é™¤ DOMAIN-SUFFIX, å’Œ DOMAIN,
        # 6. sed ç§»é™¤è¡Œå°¾æ³¨é‡Š: ç§»é™¤è¡Œå†… $ æˆ– # åŠå…¶åé¢çš„å†…å®¹
        # 7. tr: è½¬å°å†™ (æ–¹ä¾¿å»é‡)
        # 8. sed æ¸…ç†: ç§»é™¤è¡Œé¦–è¡Œå°¾ç©ºæ ¼
        # 9. awk: è¿‡æ»¤åªåŒ…å«ç‚¹å·çš„åˆæ³•åŸŸå (æ’é™¤çº¯å•è¯)
        
        cat "$input_file" \
        | tr -d '\r' \
        | grep -v "DOMAIN-KEYWORD" \
        | sed 's/^[!#].*//g' \
        | sed 's/||//g; s/\^//g' \
        | sed 's/DOMAIN-SUFFIX,//g; s/DOMAIN,//g' \
        | sed 's/[\$#].*//g' \
        | tr 'A-Z' 'a-z' \
        | sed 's/^[[:space:]]*//;s/[[:space:]]*$//' \
        | awk '/\./ {print $0}' \
        | sort -u > "$output_file"
    }

    optimize_domains() {
        local input_file=$1
        local output_file=$2

        echo "ğŸ§  æ­£åœ¨æ‰§è¡Œæ™ºèƒ½å»é‡ (ä¸»åŸŸåè¦†ç›–å­åŸŸå)..."
        
        # ç®—æ³•è¯´æ˜ï¼š
        # 1. rev: å°†åŸŸååè½¬ (google.com -> moc.elgoog)
        # 2. sort: æ’åºã€‚è¿™æ · ad.google.com (moc.elgoog.da) ä¼šç´§æŒ¨ç€ google.com (moc.elgoog)
        # 3. awk: æ¯”è¾ƒå½“å‰è¡Œæ˜¯å¦ä»¥"ä¸Šä¸€è¡Œ+."å¼€å¤´ã€‚å¦‚æœæ˜¯ï¼Œè¯´æ˜æ˜¯å­åŸŸåï¼Œä¸¢å¼ƒã€‚
        # 4. rev: ç¿»è½¬å›æ¥
        
        cat "$input_file" \
        | rev \
        | sort \
        | awk 'NR==1 {prev=$0; print; next} {if (index($0, prev ".") != 1) {print; prev=$0}}' \
        | rev \
        | sort > "$output_file"
    }

    apply_whitelist() {
        local block_file=$1
        local allow_file=$2
        local final_file=$3

        echo "ğŸ›¡ï¸  æ­£åœ¨åº”ç”¨ç™½åå•è¿‡æ»¤..."
        
        # ä½¿ç”¨ awk è¯»å–ç™½åå•åˆ°æ•°ç»„ï¼Œç„¶åéå†é»‘åå•è¿›è¡Œè¿‡æ»¤
        # æ¯” grep -vf å¿«å¾—å¤šï¼Œä¸”ä¸éœ€è¦ä¸¤ä¸ªæ–‡ä»¶éƒ½ä¸¥æ ¼æ’åº
        
        awk 'NR==FNR {whitelist[$0]=1; next} !whitelist[$0]' "$allow_file" "$block_file" > "$final_file"
    }

    # ================= ä¸»ç¨‹åºæµç¨‹ =================

    echo "=== è„šæœ¬å¼€å§‹è¿è¡Œ ==="

    # 1. ä¸‹è½½å¹¶åˆå¹¶æ‹¦æˆªè§„åˆ™
    download_files "${WORK_DIR}/raw_block.txt" "${BLOCK_URLS[@]}"

    # 2. ä¸‹è½½å¹¶åˆå¹¶ç™½åå•è§„åˆ™
    download_files "${WORK_DIR}/raw_allow.txt" "${ALLOW_URLS[@]}"

    # 3. æ¸…æ´—æ‹¦æˆªè§„åˆ™
    clean_domains "${WORK_DIR}/raw_block.txt" "${WORK_DIR}/clean_block.txt"

    # 4. æ¸…æ´—ç™½åå• (ç™½åå•ä¹Ÿå¿…é¡»æ¸…æ´—ï¼Œå¦åˆ™æ ¼å¼å¯¹ä¸ä¸Šæ— æ³•å‰”é™¤)
    clean_domains "${WORK_DIR}/raw_allow.txt" "${WORK_DIR}/clean_allow.txt"

    # 5. æ™ºèƒ½ä¼˜åŒ–æ‹¦æˆªè§„åˆ™ (å»é™¤è¢«åŒ…å«çš„å­åŸŸå)
    optimize_domains "${WORK_DIR}/clean_block.txt" "${WORK_DIR}/optimized_block.txt"

    # 6. åº”ç”¨ç™½åå•å‰”é™¤
    apply_whitelist "${WORK_DIR}/optimized_block.txt" "${WORK_DIR}/clean_allow.txt" "$OUTPUT_FILE"

    # ç»Ÿè®¡
    COUNT=$(wc -l < "$OUTPUT_FILE")
    echo "âœ… ä»»åŠ¡å®Œæˆï¼"
    echo "ğŸ“‚ è¾“å‡ºæ–‡ä»¶: $OUTPUT_FILE"
    echo "ğŸ“Š æœ€ç»ˆè§„åˆ™è¡Œæ•°: $COUNT"

    mihomo convert-ruleset domain text ADs_merged.txt ADs_merged.mrs

    # Surge compatible
    sed -i 's/+./DOMAIN-SUFFIX,/g' ADs_merged.txt

    # æ·»åŠ è®¡æ•°å’Œæ—¶é—´æˆ³
    count=$(wc -l <ADs_merged.txt)
    current_date=$(date +"%Y-%m-%d %H:%M:%S")
    temp_file=$(mktemp)
    echo "# Count: $count, Updated: $current_date" >"$temp_file"
    cat ADs_merged.txt >>"$temp_file"
    mv "$temp_file" ADs_merged.txt
  }

# å‡½æ•°ï¼šç”Ÿæˆ AIs_merged.txt
generate_ais_merged() {
  # ä¸‹è½½å¹¶åˆå¹¶è§„åˆ™
  #curl -skL https://github.com/ForestL18/rules-dat/raw/mihomo/geo/domain/ai-domain.list >>ai.txt
  #echo "" >>ai.txt
  curl -skL https://github.com/MetaCubeX/meta-rules-dat/raw/meta/geo/geosite/category-ai-!cn.list >>ai.txt
  echo "" >>ai.txt
  curl -skL https://ruleset.skk.moe/List/non_ip/ai.conf | sed 's/^DOMAIN,//g' | sed 's/^DOMAIN-SUFFIX,//g' | sed '/^#/d' >>ai.txt
  echo "" >>ai.txt
  curl -skL https://github.com/DustinWin/ruleset_geodata/raw/mihomo-ruleset/ai.list >>ai.txt
  echo "" >>ai.txt
  curl -skL https://raw.githubusercontent.com/ConnersHua/RuleGo/refs/heads/master/Surge/Ruleset/Extra/AI.list | sed 's/^DOMAIN,//g' | sed 's/^DOMAIN-SUFFIX,//g' | sed '/^#/d' >>ai.txt

  # ç§»é™¤æ³¨é‡Šå’Œç©ºè¡Œ
  cat ai.txt | sed '/^#/d' >combined_raw.txt

  # æ ‡å‡†åŒ–åŸŸå
  sed -E 's/^[\+\*\.]+//g' combined_raw.txt | grep -v '^$' >normalized.txt

  # æ’åºå¹¶å»é‡
  sort normalized.txt | uniq >unique_domains.txt

  # å…³é”®è¯æ–‡ä»¶è¿‡æ»¤
  grep -v -f "scripts/exclude-keyword.txt" unique_domains.txt >filtered_domains.txt

  # å¤„ç†åŸŸåï¼šæ·»åŠ  +. å‰ç¼€ï¼ˆDOMAIN-KEYWORD, å’Œ DOMAIN, é™¤å¤–ï¼‰
  awk '{
      if ($0 ~ /^DOMAIN-KEYWORD,/ || $0 ~ /^DOMAIN,/) {
          print $0
      } else {
          print "+." $0
      }
  }' filtered_domains.txt >AIs_merged.txt

  mihomo convert-ruleset domain text AIs_merged.txt AIs_merged.mrs

  # Surge compatible
  sed -i 's/+./DOMAIN-SUFFIX,/g' AIs_merged.txt

  # æ·»åŠ è®¡æ•°å’Œæ—¶é—´æˆ³
  count=$(wc -l <AIs_merged.txt)
  current_date=$(date +"%Y-%m-%d %H:%M:%S")
  temp_file=$(mktemp)
  echo "# Count: $count, Updated: $current_date" >"$temp_file"
  cat AIs_merged.txt >>"$temp_file"
  mv "$temp_file" AIs_merged.txt
}

# å‡½æ•°ï¼šç”Ÿæˆ Fake_IP_Fliter_merged.txt
generate_Fake_IP_Fliter_merged() {
  # ä¸‹è½½å¹¶åˆå¹¶è§„åˆ™
  echo "" >>Fake_IP_Fliter.txt
  curl -skL https://raw.githubusercontent.com/vernesong/OpenClash/refs/heads/master/luci-app-openclash/root/etc/openclash/custom/openclash_custom_fake_filter.list >>Fake_IP_Fliter.txt
  echo "" >>Fake_IP_Fliter.txt
  curl -skL https://raw.githubusercontent.com/juewuy/ShellCrash/dev/public/fake_ip_filter.list >>Fake_IP_Fliter.txt
  echo "" >>Fake_IP_Fliter.txt
  curl -skL https://raw.githubusercontent.com/DustinWin/ruleset_geodata/refs/heads/mihomo-ruleset/fakeip-filter.list >>Fake_IP_Fliter.txt
  echo "" >>Fake_IP_Fliter.txt
  curl -skL https://raw.githubusercontent.com/wuiiled/Wuiiled_Setup/refs/heads/master/scripts/fake-ip-addon.txt >>Fake_IP_Fliter.txt

  # ç§»é™¤æ³¨é‡Šå’Œç©ºè¡Œ
  #cat Fake_IP_Fliter.txt | sed '/^[#!]/d' >Fake_IP_Fliter_combined_raw.txt

  # ç§»é™¤æ³¨é‡Šå’Œç©ºè¡Œå¹¶æ ‡å‡†åŒ–åŸŸå
  #sed -E 's/^[\+\*\.]+//g' Fake_IP_Fliter_combined_raw.txt | grep -v '^$' | tr '[:upper:]' '[:lower:]' | sed 's/[[:space:]]*$//' > Fake_IP_Fliter_normalized.txt
  tr -d '\r' < Fake_IP_Fliter.txt | sed -E '/^[[:space:]]*(#|$)/d; s/^[[:space:]]+//; s/[[:space:]]+$//' > Fake_IP_Fliter_combined_clean.txt

  # æ’åºå¹¶å»é‡
  sort Fake_IP_Fliter_combined_clean.txt | uniq >Fake_IP_Fliter_merged.txt

  # å¤„ç†åŸŸåï¼šæ·»åŠ  +. å‰ç¼€ï¼ˆDOMAIN-KEYWORD é™¤å¤–ï¼‰
  #awk '{
  #    if ($0 ~ /^DOMAIN-KEYWORD/) {
  #        print $0
  #    } else {
  #        print "+." $0
  #    }
  #}' Fake_IP_Fliter_domains.txt >Fake_IP_Fliter_merged.txt

  mihomo convert-ruleset domain text Fake_IP_Fliter_merged.txt Fake_IP_Fliter_merged.mrs

  # Surge compatible
  #sed -i 's/+./DOMAIN-SUFFIX,/g' Fake_IP_Fliter_merged.txt

  # æ·»åŠ è®¡æ•°å’Œæ—¶é—´æˆ³
  count=$(wc -l <Fake_IP_Fliter_merged.txt)
  current_date=$(date +"%Y-%m-%d %H:%M:%S")
  temp_file=$(mktemp)
  echo "# Count: $count, Updated: $current_date" >"$temp_file"
  cat Fake_IP_Fliter_merged.txt >>"$temp_file"
  mv "$temp_file" Fake_IP_Fliter_merged.txt
}

# ä¸»å‡½æ•°
main() {
  if [ "$1" == "ads" ]; then
    generate_ads_merged
  elif [ "$1" == "ais" ]; then
    generate_ais_merged
  elif [ "$1" == "fakeip" ]; then
    generate_Fake_IP_Fliter_merged
  else
    echo "Usage: $0 [ads|ais|fakeip]"
    exit 1
  fi
}

# è°ƒç”¨ä¸»å‡½æ•°
main "$@"
